#!/bin/bash
export WANDB_DISABLED=true
export WANDB_MODE=disabled
export CUDA_VISIBLE_DEVICES=0

# æ”¹è¿›çš„å¾®è°ƒè®­ç»ƒè„šæœ¬
LOG_FILE="improved_finetune_$(date +%Y%m%d_%H%M%S).log"
echo "Starting improved finetune strategy..."
echo "Strategy: Conservative training with proper initialization"
echo "All output will be logged to: $LOG_FILE"

# æ£€æŸ¥æ˜¯å¦åœ¨screenä¸­è¿è¡Œ
if [ -z "$STY" ]; then
    echo "Not running in screen. Starting screen session..."
    echo "Creating screen session 'improved_finetune'..."
    
    # åˆ›å»ºscreenä¼šè¯å¹¶è¿è¡Œè„šæœ¬
    screen -dmS improved_finetune bash -c "
        echo 'Screen session started at: \$(date)'
        echo 'Working directory: \$(pwd)'
        echo 'Starting improved finetune training...'
        bash $0
        echo 'Training completed at: \$(date)'
        echo 'Press any key to exit...'
        read -n 1
    "
    
    echo "âœ… Training started in screen session 'improved_finetune'"
    echo ""
    echo "ðŸ“‹ Useful commands:"
    echo "  â€¢ Attach to session:    screen -r improved_finetune"
    echo "  â€¢ Detach from session:  Ctrl+A, then D"
    echo "  â€¢ List all sessions:    screen -ls"
    echo "  â€¢ Kill session:         screen -S improved_finetune -X quit"
    echo ""
    echo "ðŸ“ Log file: $LOG_FILE"
    echo "ðŸ“Š Monitor progress: tail -f $LOG_FILE"
    echo ""
    echo "The training is now running in the background!"
    exit 0
fi

echo "Running in screen session: $STY"

# è®¾ç½®æ—¥å¿—è¾“å‡º
exec > >(tee -a $LOG_FILE) 2>&1

echo 'Starting improved finetune training...'
echo "Log file: $LOG_FILE"
echo "Timestamp: $(date)"

# è®¾ç½®é”™è¯¯å¤„ç†
set -e
trap 'echo "Error occurred at line $LINENO. Check log file: $LOG_FILE"' ERR

# åˆ›å»ºcheckpointsç›®å½•
mkdir -p checkpoints

# ===========================================
# æ­¥éª¤1ï¼šè®¾ç½®æ”¹è¿›çš„è®­ç»ƒç­–ç•¥
# ===========================================
echo '=== STEP 1: Setting up improved training strategy ==='
echo "Setup started at: $(date)"

/home/shane/miniforge3/condabin/conda run -n uva python improved_finetune_strategy.py

echo 'Improved strategy setup completed!'
echo "Setup finished at: $(date)"

# ===========================================
# æ­¥éª¤2ï¼šé˜¶æ®µ1è®­ç»ƒ - Local Attention Only
# ===========================================
echo '=== STEP 2: Phase 1 - Local Attention Only ==='
echo "Phase 1 started at: $(date)"

# é˜¶æ®µ1è®­ç»ƒ - æ›´ä¿å®ˆçš„è®¾ç½®
echo 'Starting Phase 1 training with improved settings...'
/home/shane/miniforge3/condabin/conda run -n uva accelerate launch --num_processes=1 train.py \
    --config-dir=unified_video_action/config \
    --config-name=uva_pusht \
    model.policy.autoregressive_model_params.pretrained_model_path='checkpoints/improved_phase1_local_only.ckpt' \
    model.policy.action_model_params.predict_action=True \
    training.num_epochs=50 \
    model.policy.optimizer.learning_rate=5e-5 \
    dataloader.batch_size=8 \
    logging.mode=offline \
    hydra.run.dir='checkpoints/improved_phase1_local_attention'

echo 'Phase 1 training completed!'
echo "Phase 1 finished at: $(date)"

# è¯„ä¼°é˜¶æ®µ1æ€§èƒ½
echo 'Evaluating Phase 1 performance...'
/home/shane/miniforge3/condabin/conda run -n uva python evaluate_performance.py \
    --checkpoint='checkpoints/improved_phase1_local_attention/checkpoints/topk.ckpt' \
    --phase='Improved_Phase_1_Local_Attention'

# æ€§èƒ½æ£€æŸ¥
echo 'Checking Phase 1 performance...'
/home/shane/miniforge3/condabin/conda run -n uva python check_performance_degradation.py \
    --current-checkpoint='checkpoints/improved_phase1_local_attention/checkpoints/topk.ckpt' \
    --baseline-checkpoint='checkpoints/pusht.ckpt' \
    --phase-name='Phase_1' \
    --threshold=0.1

if [ $? -ne 0 ]; then
    echo "âŒ Phase 1 performance check failed!"
    echo "Consider adjusting learning rate or initialization"
    echo "Continuing to Phase 2 with caution..."
fi

# ===========================================
# æ­¥éª¤3ï¼šé˜¶æ®µ2è®­ç»ƒ - Local Attention + Partial Encoder
# ===========================================
echo '=== STEP 3: Phase 2 - Local Attention + Partial Encoder ==='
echo "Phase 2 started at: $(date)"

# åŠ è½½é˜¶æ®µ1çš„ç»“æžœå¹¶è®¾ç½®é˜¶æ®µ2
cat > setup_improved_phase2.py << 'EOF'
import torch
import sys
import os
sys.path.append("/home/shane/code_b/unified_video_action")

import hydra
from omegaconf import OmegaConf
from unified_video_action.policy.unified_video_action_policy import UnifiedVideoActionPolicy

# åŠ è½½é…ç½®
from hydra import initialize, compose
with initialize(config_path="unified_video_action/config"):
    cfg = compose(config_name="uva_pusht")

OmegaConf.resolve(cfg)
policy = UnifiedVideoActionPolicy(**cfg.model.policy, normalizer_type=cfg.task.dataset.normalizer_type, task_name=cfg.task.name)

# åŠ è½½é˜¶æ®µ1çš„checkpoint
checkpoint_path = "checkpoints/improved_phase1_local_attention/checkpoints/topk.ckpt"
if not os.path.exists(checkpoint_path):
    print(f"ERROR: Phase 1 checkpoint not found: {checkpoint_path}")
    exit(1)

checkpoint = torch.load(checkpoint_path, map_location="cpu")
if "state_dict" in checkpoint:
    policy.load_state_dict(checkpoint["state_dict"])
elif "state_dicts" in checkpoint and "ema_model" in checkpoint["state_dicts"]:
    ema_state_dict = checkpoint["state_dicts"]["ema_model"]
    model_state_dict = {k[6:]: v for k, v in ema_state_dict.items() if k.startswith("model.")}
    policy.load_state_dict(model_state_dict, strict=False)
else:
    policy.load_state_dict(checkpoint, strict=False)

print(f"Successfully loaded Phase 1 checkpoint from: {checkpoint_path}")

# å†»ç»“æ‰€æœ‰å‚æ•°
for param in policy.model.parameters():
    param.requires_grad = False

# è§£å†»å‚æ•°ï¼šlocal attention + å°‘é‡encoderå±‚
phase2_params = []
for name, param in policy.model.named_parameters():
    should_unfreeze = False
    
    # Local attentionå‚æ•°ï¼ˆä¿æŒè§£å†»ï¼‰
    if any(keyword in name for keyword in ['local_causal_blocks', 'feature_fusion', 'gate']):
        should_unfreeze = True
    
    # è§£å†»encoderçš„æœ€åŽ2å±‚ï¼ˆlayer 10-11ï¼‰
    elif ('encoder_blocks' in name and 
          any(f'.{i}.' in name for i in range(10, 12))):
        should_unfreeze = True
    
    # è§£å†»encoder norm
    elif 'encoder_norm' in name:
        should_unfreeze = True
    
    if should_unfreeze:
        param.requires_grad = True
        phase2_params.append(name)

print(f"Phase 2 unfrozen parameters: {len(phase2_params)}")

# ä¿å­˜æ¨¡åž‹
checkpoint = {
    "state_dicts": {
        "ema_model": {f"model.{k}": v for k, v in policy.state_dict().items()}
    }
}
torch.save(checkpoint, "checkpoints/improved_phase2_partial.ckpt")
print("Phase 2 setup completed")
EOF

/home/shane/miniforge3/condabin/conda run -n uva python setup_improved_phase2.py

# é˜¶æ®µ2è®­ç»ƒ
echo 'Starting Phase 2 training...'
/home/shane/miniforge3/condabin/conda run -n uva accelerate launch --num_processes=1 train.py \
    --config-dir=unified_video_action/config \
    --config-name=uva_pusht \
    model.policy.autoregressive_model_params.pretrained_model_path='checkpoints/improved_phase2_partial.ckpt' \
    model.policy.action_model_params.predict_action=True \
    training.num_epochs=30 \
    model.policy.optimizer.learning_rate=3e-5 \
    dataloader.batch_size=8 \
    logging.mode=offline \
    hydra.run.dir='checkpoints/improved_phase2_partial_finetune'

echo 'Phase 2 training completed!'

# è¯„ä¼°é˜¶æ®µ2æ€§èƒ½
echo 'Evaluating Phase 2 performance...'
/home/shane/miniforge3/condabin/conda run -n uva python evaluate_performance.py \
    --checkpoint='checkpoints/improved_phase2_partial_finetune/checkpoints/topk.ckpt' \
    --phase='Improved_Phase_2_Partial_Encoder'

# ===========================================
# æ­¥éª¤4ï¼šé˜¶æ®µ3è®­ç»ƒ - Local Attention + Extended Encoder
# ===========================================
echo '=== STEP 4: Phase 3 - Local Attention + Extended Encoder ==='
echo "Phase 3 started at: $(date)"

# åŠ è½½é˜¶æ®µ2çš„ç»“æžœå¹¶è®¾ç½®é˜¶æ®µ3
cat > setup_improved_phase3.py << 'EOF'
import torch
import sys
import os
sys.path.append("/home/shane/code_b/unified_video_action")

import hydra
from omegaconf import OmegaConf
from unified_video_action.policy.unified_video_action_policy import UnifiedVideoActionPolicy

# åŠ è½½é…ç½®
from hydra import initialize, compose
with initialize(config_path="unified_video_action/config"):
    cfg = compose(config_name="uva_pusht")

OmegaConf.resolve(cfg)
policy = UnifiedVideoActionPolicy(**cfg.model.policy, normalizer_type=cfg.task.dataset.normalizer_type, task_name=cfg.task.name)

# åŠ è½½é˜¶æ®µ2çš„checkpoint
checkpoint_path = "checkpoints/improved_phase2_partial_finetune/checkpoints/topk.ckpt"
if not os.path.exists(checkpoint_path):
    print(f"ERROR: Phase 2 checkpoint not found: {checkpoint_path}")
    exit(1)

checkpoint = torch.load(checkpoint_path, map_location="cpu")
if "state_dict" in checkpoint:
    policy.load_state_dict(checkpoint["state_dict"])
elif "state_dicts" in checkpoint and "ema_model" in checkpoint["state_dicts"]:
    ema_state_dict = checkpoint["state_dicts"]["ema_model"]
    model_state_dict = {k[6:]: v for k, v in ema_state_dict.items() if k.startswith("model.")}
    policy.load_state_dict(model_state_dict, strict=False)
else:
    policy.load_state_dict(checkpoint, strict=False)

print(f"Successfully loaded Phase 2 checkpoint from: {checkpoint_path}")

# å†»ç»“æ‰€æœ‰å‚æ•°
for param in policy.model.parameters():
    param.requires_grad = False

# è§£å†»å‚æ•°ï¼šlocal attention + æ›´å¤šencoderå±‚
phase3_params = []
for name, param in policy.model.named_parameters():
    should_unfreeze = False
    
    # Local attentionå‚æ•°ï¼ˆä¿æŒè§£å†»ï¼‰
    if any(keyword in name for keyword in ['local_causal_blocks', 'feature_fusion', 'gate']):
        should_unfreeze = True
    
    # è§£å†»encoderçš„åŽåŠéƒ¨åˆ†ï¼ˆlayer 6-11ï¼‰
    elif ('encoder_blocks' in name and 
          any(f'.{i}.' in name for i in range(6, 12))):
        should_unfreeze = True
    
    # è§£å†»encoder norm
    elif 'encoder_norm' in name:
        should_unfreeze = True
    
    # è§£å†»decoderçš„å‰å‡ å±‚
    elif ('decoder_blocks' in name and 
          any(f'.{i}.' in name for i in range(0, 4))):
        should_unfreeze = True
    
    if should_unfreeze:
        param.requires_grad = True
        phase3_params.append(name)

print(f"Phase 3 unfrozen parameters: {len(phase3_params)}")

# ä¿å­˜æ¨¡åž‹
checkpoint = {
    "state_dicts": {
        "ema_model": {f"model.{k}": v for k, v in policy.state_dict().items()}
    }
}
torch.save(checkpoint, "checkpoints/improved_phase3_extended.ckpt")
print("Phase 3 setup completed")
EOF

/home/shane/miniforge3/condabin/conda run -n uva python setup_improved_phase3.py

# é˜¶æ®µ3è®­ç»ƒ
echo 'Starting Phase 3 training...'
/home/shane/miniforge3/condabin/conda run -n uva accelerate launch --num_processes=1 train.py \
    --config-dir=unified_video_action/config \
    --config-name=uva_pusht \
    model.policy.autoregressive_model_params.pretrained_model_path='checkpoints/improved_phase3_extended.ckpt' \
    model.policy.action_model_params.predict_action=True \
    training.num_epochs=20 \
    model.policy.optimizer.learning_rate=1e-5 \
    dataloader.batch_size=8 \
    logging.mode=offline \
    hydra.run.dir='checkpoints/improved_phase3_extended_finetune'

echo 'Phase 3 training completed!'

# è¯„ä¼°é˜¶æ®µ3æ€§èƒ½
echo 'Evaluating Phase 3 performance...'
/home/shane/miniforge3/condabin/conda run -n uva python evaluate_performance.py \
    --checkpoint='checkpoints/improved_phase3_extended_finetune/checkpoints/topk.ckpt' \
    --phase='Improved_Phase_3_Extended_Encoder'

echo '=== IMPROVED FINETUNE TRAINING COMPLETED! ==='
echo 'Final model saved in: checkpoints/improved_phase3_extended_finetune/checkpoints/topk.ckpt'
echo ''
echo '=== TRAINING SUMMARY ==='
echo 'Strategy: Conservative progressive training with proper initialization'
echo 'Phase 1: Local attention only (50 epochs, 5e-5 lr)'
echo 'Phase 2: + Encoder layers 10-11 (30 epochs, 3e-5 lr)'  
echo 'Phase 3: + Encoder layers 6-11 + Decoder layers 0-3 (20 epochs, 1e-5 lr)'
echo ''
echo 'Key improvements:'
echo '- Proper initialization of local attention parameters'
echo '- Increased training epochs for better convergence'
echo '- Lower learning rates to prevent instability'
echo '- Conservative parameter unfreezing strategy'
echo '- Better performance monitoring'

# ç”Ÿæˆæ€§èƒ½æ¯”è¾ƒæŠ¥å‘Š
echo 'Generating performance comparison report...'
/home/shane/miniforge3/condabin/conda run -n uva python compare_performance.py \
    --checkpoints-dir='checkpoints' \
    --output-report='improved_finetune_report.txt'

echo "Improved finetune training completed"
echo "All output is logged to: $LOG_FILE"
echo "To view real-time progress: tail -f $LOG_FILE"
